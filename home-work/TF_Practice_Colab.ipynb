{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e437082",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/alexander-toschev/ml-cs-intro/blob/main/home-work/HW_END_TO_END.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b29f89b",
   "metadata": {},
   "source": [
    "# TensorFlow Practice Notebook ðŸ‡¬ðŸ‡§\n",
    "\n",
    "This Colab notebook is for **hands-on practice** with TensorFlow 2.x and `tf.keras`.\n",
    "\n",
    "It contains several *guided exercises*:\n",
    "1. Basic TensorFlow tensors & operations  \n",
    "2. Simple dense neural network on MNIST  \n",
    "3. Convolutional neural network (CNN) on MNIST  \n",
    "4. `tf.data` input pipelines  \n",
    "5. Custom training loop with `tf.GradientTape`  \n",
    "\n",
    "> Fill in all places marked with `# TODO` and run the cells.  \n",
    "> There is **no auto-grading** here â€” this is pure practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb666a5",
   "metadata": {},
   "source": [
    "## 0. Setup and imports\n",
    "\n",
    "In this section we import TensorFlow and load the MNIST dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f9b77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Load MNIST\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize to [0, 1]\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Add channel dimension: (N, 28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "num_classes = 10\n",
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "print(\"Train shape:\", x_train.shape, \"Test shape:\", x_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887d8a68",
   "metadata": {},
   "source": [
    "## 1. Practice: Basic tensors and operations\n",
    "\n",
    "In this exercise, you will:\n",
    "\n",
    "- Create random tensors\n",
    "- Compute basic statistics with TensorFlow ops\n",
    "- Reshape and slice tensors\n",
    "\n",
    "### Task 1.1\n",
    "\n",
    "Create a function `create_random_tensor(n)` that:\n",
    "\n",
    "- Takes an integer `n`\n",
    "- Returns a 1D tensor of shape `(n,)` with values sampled from a normal distribution `N(0, 1)` **using TensorFlow**\n",
    "\n",
    "### Task 1.2\n",
    "\n",
    "Create a function `describe_tensor(x)` that:\n",
    "\n",
    "- Takes a 1D tensor `x`\n",
    "- Prints:\n",
    "  - shape\n",
    "  - dtype\n",
    "  - mean\n",
    "  - standard deviation\n",
    "  - min and max values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3247497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement create_random_tensor and describe_tensor\n",
    "\n",
    "def create_random_tensor(n: int) -> tf.Tensor:\n",
    "    \"\"\"Return a 1D tensor of shape (n,) sampled from N(0, 1).\"\"\"\n",
    "    # TODO: use TensorFlow to create random normal values\n",
    "    # hint: tf.random.normal(...)\n",
    "    x = tf.random.normal(shape=(n,), mean=0.0, stddev=1.0, seed=SEED)\n",
    "    return x\n",
    "\n",
    "\n",
    "def describe_tensor(x: tf.Tensor) -> None:\n",
    "    \"\"\"Print basic information about the tensor x.\"\"\"\n",
    "    # TODO: print shape, dtype, mean, std, min, max using TF ops\n",
    "    print(\"Shape:\", x.shape)\n",
    "    print(\"Dtype:\", x.dtype)\n",
    "    mean = tf.reduce_mean(x)\n",
    "    std = tf.math.reduce_std(x)\n",
    "    min_v = tf.reduce_min(x)\n",
    "    max_v = tf.reduce_max(x)\n",
    "    print(\"Mean:\", float(mean.numpy()))\n",
    "    print(\"Std: \", float(std.numpy()))\n",
    "    print(\"Min: \", float(min_v.numpy()))\n",
    "    print(\"Max: \", float(max_v.numpy()))\n",
    "\n",
    "\n",
    "# Try it\n",
    "x = create_random_tensor(10)\n",
    "describe_tensor(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d536d7c",
   "metadata": {},
   "source": [
    "## 2. Practice: Dense neural network on MNIST\n",
    "\n",
    "Now you will build a simple **fully-connected (dense) network** for MNIST classification.\n",
    "\n",
    "### Task 2.1 â€” Build the model\n",
    "\n",
    "Implement `build_dense_model(input_shape, num_classes)`:\n",
    "\n",
    "- Input: `input_shape = (28, 28, 1)`\n",
    "- Architecture (recommended):\n",
    "  - `Flatten`\n",
    "  - `Dense(256, activation=\"relu\")`\n",
    "  - `Dense(128, activation=\"relu\")`\n",
    "  - `Dense(num_classes, activation=\"softmax\")`\n",
    "- Compile with:\n",
    "  - `Adam(1e-3)`\n",
    "  - `\"sparse_categorical_crossentropy\"`\n",
    "  - metric `\"accuracy\"`\n",
    "\n",
    "### Task 2.2 â€” Train the model\n",
    "\n",
    "Train the model for **5 epochs** with batch size 128 and use `validation_split=0.1`.\n",
    "\n",
    "Observe:\n",
    "- Training / validation accuracy\n",
    "- Underfitting or overfitting?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9790bdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement build_dense_model and train it\n",
    "\n",
    "def build_dense_model(input_shape, num_classes):\n",
    "    \"\"\"Build and compile a dense neural network for MNIST.\"\"\"\n",
    "    # TODO: define the model using tf.keras\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = keras.layers.Flatten()(inputs)\n",
    "    x = keras.layers.Dense(256, activation=\"relu\")(x)\n",
    "    x = keras.layers.Dense(128, activation=\"relu\")(x)\n",
    "    outputs = keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "dense_model = build_dense_model(input_shape, num_classes)\n",
    "dense_model.summary()\n",
    "\n",
    "history_dense = dense_model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_split=0.1,\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e2996c",
   "metadata": {},
   "source": [
    "## 3. Practice: Convolutional Neural Network (CNN) on MNIST\n",
    "\n",
    "Now build a small **CNN** which usually works better for images.\n",
    "\n",
    "### Task 3.1 â€” Build a CNN model\n",
    "\n",
    "Implement `build_cnn_model(input_shape, num_classes)` with:\n",
    "\n",
    "- `Conv2D(32, kernel_size=3, activation=\"relu\")`\n",
    "- `MaxPooling2D()`\n",
    "- `Conv2D(64, kernel_size=3, activation=\"relu\")`\n",
    "- `MaxPooling2D()`\n",
    "- `Flatten`\n",
    "- `Dense(128, activation=\"relu\")`\n",
    "- `Dense(num_classes, activation=\"softmax\")`\n",
    "\n",
    "Compile the model with the same settings as before.\n",
    "\n",
    "### Task 3.2 â€” Train and compare\n",
    "\n",
    "Train for **5 epochs** and compare validation accuracy with the dense model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70422b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement build_cnn_model and train it\n",
    "\n",
    "def build_cnn_model(input_shape, num_classes):\n",
    "    \"\"\"Build and compile a simple CNN for MNIST.\"\"\"\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = keras.layers.Conv2D(32, 3, activation=\"relu\")(inputs)\n",
    "    x = keras.layers.MaxPooling2D()(x)\n",
    "    x = keras.layers.Conv2D(64, 3, activation=\"relu\")(x)\n",
    "    x = keras.layers.MaxPooling2D()(x)\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    x = keras.layers.Dense(128, activation=\"relu\")(x)\n",
    "    outputs = keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "cnn_model = build_cnn_model(input_shape, num_classes)\n",
    "cnn_model.summary()\n",
    "\n",
    "history_cnn = cnn_model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_split=0.1,\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d21587c",
   "metadata": {},
   "source": [
    "## 4. Practice: `tf.data` input pipeline\n",
    "\n",
    "In this section, you will build a `tf.data.Dataset` for MNIST and use it with `.fit()`.\n",
    "\n",
    "### Task 4.1 â€” Create datasets\n",
    "\n",
    "Implement `make_dataset(x, y, batch_size)` that:\n",
    "\n",
    "- creates a `tf.data.Dataset` from `(x, y)`\n",
    "- shuffles with buffer size `10000`\n",
    "- batches with `batch_size`\n",
    "- prefetches with `tf.data.AUTOTUNE`\n",
    "\n",
    "Create:\n",
    "\n",
    "- `train_ds` from `x_train, y_train`\n",
    "- `test_ds` from `x_test, y_test` (without shuffling)\n",
    "\n",
    "### Task 4.2 â€” Train with `tf.data`\n",
    "\n",
    "Train the **CNN model** from the previous section using `train_ds` instead of NumPy arrays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499889e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement make_dataset and train with tf.data\n",
    "\n",
    "def make_dataset(x, y, batch_size: int, shuffle: bool = True) -> tf.data.Dataset:\n",
    "    ds = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=10000, seed=SEED)\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "train_ds = make_dataset(x_train, y_train, batch_size=batch_size, shuffle=True)\n",
    "test_ds = make_dataset(x_test, y_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Rebuild a fresh CNN model\n",
    "cnn_model_ds = build_cnn_model(input_shape, num_classes)\n",
    "\n",
    "history_cnn_ds = cnn_model_ds.fit(\n",
    "    train_ds,\n",
    "    epochs=5,\n",
    "    validation_data=test_ds,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c532744f",
   "metadata": {},
   "source": [
    "## 5. Practice: Custom training loop with `tf.GradientTape`\n",
    "\n",
    "Here we will train a small network for a **toy regression problem** using a fully custom loop.\n",
    "\n",
    "We will:\n",
    "\n",
    "1. Generate synthetic data for `y = 3x + noise`\n",
    "2. Build a small dense model with Keras\n",
    "3. Write the training loop using `tf.GradientTape`\n",
    "\n",
    "### Task 5.1 â€” Generate data\n",
    "\n",
    "- Create 1000 points `x` uniformly in [-1, 1]\n",
    "- Compute `y = 3 * x + noise`, where noise is normal with std 0.1\n",
    "\n",
    "### Task 5.2 â€” Build model\n",
    "\n",
    "- Simple `Sequential` model:\n",
    "  - `Dense(8, activation=\"relu\", input_shape=(1,))`\n",
    "  - `Dense(1)`\n",
    "\n",
    "### Task 5.3 â€” Custom loop\n",
    "\n",
    "For several epochs:\n",
    "\n",
    "- for each batch:\n",
    "  - run forward pass\n",
    "  - compute MSE loss\n",
    "  - compute gradients w.r.t. trainable variables\n",
    "  - apply gradients with Adam optimizer\n",
    "\n",
    "Observe how loss decreases and how learned weight compares to 3.0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87587ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement the custom training loop for regression\n",
    "\n",
    "# 5.1 Generate data\n",
    "n_samples = 1000\n",
    "x_reg = np.random.uniform(-1.0, 1.0, size=(n_samples, 1)).astype(\"float32\")\n",
    "noise = np.random.normal(loc=0.0, scale=0.1, size=(n_samples, 1)).astype(\"float32\")\n",
    "y_reg = 3.0 * x_reg + noise\n",
    "\n",
    "# Build dataset\n",
    "batch_size_reg = 32\n",
    "reg_ds = tf.data.Dataset.from_tensor_slices((x_reg, y_reg))\n",
    "reg_ds = reg_ds.shuffle(buffer_size=1000, seed=SEED).batch(batch_size_reg)\n",
    "\n",
    "# 5.2 Build model\n",
    "reg_model = keras.Sequential([\n",
    "    keras.layers.Dense(8, activation=\"relu\", input_shape=(1,)),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-2)\n",
    "loss_fn = keras.losses.MeanSquaredError()\n",
    "\n",
    "# 5.3 Custom training loop\n",
    "n_epochs_reg = 20\n",
    "\n",
    "for epoch in range(n_epochs_reg):\n",
    "    epoch_loss = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for x_batch, y_batch in reg_ds:\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = reg_model(x_batch, training=True)\n",
    "            loss_value = loss_fn(y_batch, y_pred)\n",
    "\n",
    "        grads = tape.gradient(loss_value, reg_model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, reg_model.trainable_variables))\n",
    "\n",
    "        epoch_loss += float(loss_value.numpy())\n",
    "        n_batches += 1\n",
    "\n",
    "    epoch_loss /= n_batches\n",
    "    print(f\"Epoch {epoch+1:02d}: loss = {epoch_loss:.4f}\")\n",
    "\n",
    "# Inspect learned weight (approx. 3.0)\n",
    "for var in reg_model.trainable_variables:\n",
    "    print(var.name, var.numpy())\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
