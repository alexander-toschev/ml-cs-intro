{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexander-toschev/ml-cs-intro/blob/main/home-work/HW_TensorFlow_Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "190a4f42",
      "metadata": {
        "id": "190a4f42"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexander-toschev/ml-cs-intro/blob/main/home-work/HW_TensorFlow_Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fa479b2",
      "metadata": {
        "id": "3fa479b2"
      },
      "source": [
        "# TensorFlow & tf.keras ‚Äî Homework\n",
        "\n",
        "This notebook contains **auto-graded tasks** about core TensorFlow and the `tf.keras` high-level API.\n",
        "\n",
        "Fill in the cells marked with `# TODO` and run the **TEST CELL** below each task.\n",
        "\n",
        "- Language: English (code + comments)\n",
        "- Topic focus: **TensorFlow basics** + **end-to-end training with tf.keras**\n",
        "- Total: **100 points**\n",
        "\n",
        "After each test cell you will see:\n",
        "- Points for this task\n",
        "- Cumulative **TOTAL POINTS**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f8ffb3a1",
      "metadata": {
        "id": "f8ffb3a1",
        "outputId": "0ef76d64-796d-4f84-f6a5-46b7da82c787",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úî Student Info OK\n",
            "Student: Doe John\n"
          ]
        }
      ],
      "source": [
        "# @title 1) Student Info & Config\n",
        "# All code comments are in English.\n",
        "\n",
        "\n",
        "# === –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –ó–ê–ü–û–õ–ù–ò–¢–¨ ===\n",
        "full_name = \"Doe John\"     # –Ω–∞–ø—Ä–∏–º–µ—Ä: \"–¢–æ—â–µ–≤ –ê–ª–µ–∫—Å–∞–Ω–¥—Ä\"\n",
        "student_group = \"11-111\"      # –Ω–∞–ø—Ä–∏–º–µ—Ä: \"208\"\n",
        "assignment_id = \"HW_TENSORFLOW_BASICS\"\n",
        "assert full_name != \"–§–∞–º–∏–ª–∏—è –ò–º—è\", \"–ó–∞–ø–æ–ª–Ω–∏—Ç–µ full_name\"\n",
        "assert student_group != \"–ì—Ä—É–ø–ø–∞\", \"–ó–∞–ø–æ–ª–Ω–∏—Ç–µ student_group\"\n",
        "print(\"‚úî Student Info OK\")\n",
        "\n",
        "# Typical human accuracy (benchmark) for MNIST may be ~97-99%.\n",
        "\n",
        "print(\"Student:\", full_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime, timezone, timedelta\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def _close(a, b, tol=1e-8):\n",
        "    return np.allclose(a, b, atol=tol)\n",
        "\n",
        "def _arr_equal(a, b):\n",
        "    a = np.array(a)\n",
        "    b = np.array(b)\n",
        "    return a.shape == b.shape and np.array_equal(a, b)\n",
        "# –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –æ–∫–Ω–∞ –ø—Ä–∏—ë–º–∞ (–ø—Ä–∏–º–µ—Ä):\n",
        "\n",
        "start_at_iso = \"2025-12-08T09:00-04:00\"  #@param {type:\"string\"}\n",
        "due_at_iso   = \"2025-12-15T23:59-04:00\"  #@param {type:\"string\"}\n",
        "start_dt = datetime.fromisoformat(start_at_iso)\n",
        "due_dt   = datetime.fromisoformat(due_at_iso)\n",
        "# –î–ª—è –ø—Ä–æ—Ç–æ–∫–æ–ª–∞: –≤—Ä–µ–º—è —Å–¥–∞—á–∏ –±–µ—Ä—ë–º —Ç–µ–∫—É—â–µ–µ (–º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ mtime —Ñ–∞–π–ª–∞)\n",
        "import os\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "# üìÖ Add submission date based on file modification time\n",
        "try:\n",
        "    nb_path = __file__ if \"__file__\" in globals() else \"HW_TensorFlow_Basics.ipynb\"\n",
        "    mtime = os.path.getmtime(nb_path)\n",
        "    submission_dt = datetime.fromtimestamp(mtime, tz=timezone.utc)\n",
        "except Exception:\n",
        "    submission_dt = datetime.utcnow().replace(tzinfo=timezone.utc)\n",
        "\n",
        "def penalty_fraction(start_dt, due_dt, submission_dt):\n",
        "    \"\"\"–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–æ–ª—é —à—Ç—Ä–∞—Ñ–∞ [0..1].\n",
        "    0 ‚Äî –±–µ–∑ —à—Ç—Ä–∞—Ñ–∞ (<= due_dt). –õ–∏–Ω–µ–π–Ω–æ —Ä–∞—Å—Ç—ë—Ç –æ—Ç due_dt –∫ due_dt + (due_dt - start_dt).\n",
        "    –ù–µ –≤—ã—Ö–æ–¥–∏—Ç –∑–∞ 1.0.\n",
        "    \"\"\"\n",
        "    if submission_dt <= due_dt:\n",
        "        return 0.0\n",
        "    total = (due_dt - start_dt).total_seconds()\n",
        "    late  = (submission_dt - due_dt).total_seconds()\n",
        "    if total <= 0:\n",
        "        return 1.0 if late > 0 else 0.0\n",
        "    return min(1.0, max(0.0, late / total))\n",
        "\n",
        "print(f\"–û–∫–Ω–æ –ø—Ä–∏—ë–º–∞: {start_dt.isoformat()} ‚Äî {due_dt.isoformat()} (UTC)\")\n",
        "print(f\"–í—Ä–µ–º—è —Å–¥–∞—á–∏: {submission_dt.isoformat()} (UTC)\")"
      ],
      "metadata": {
        "id": "SA070k09eVB0",
        "outputId": "1c521bb5-9e21-466b-9981-4b7b29c0b00f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "SA070k09eVB0",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–û–∫–Ω–æ –ø—Ä–∏—ë–º–∞: 2025-12-08T09:00:00-04:00 ‚Äî 2025-12-15T23:59:00-04:00 (UTC)\n",
            "–í—Ä–µ–º—è —Å–¥–∞—á–∏: 2025-12-08T15:00:04.130500+00:00 (UTC)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-695501909.py:29: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  submission_dt = datetime.utcnow().replace(tzinfo=timezone.utc)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "711e59bc",
      "metadata": {
        "id": "711e59bc"
      },
      "outputs": [],
      "source": [
        "# Global score storage (do not modify)\n",
        "SCORES = {}\n",
        "\n",
        "def _set_score(task_name, points, max_points):\n",
        "    SCORES[task_name] = min(points, max_points)\n",
        "    total = sum(SCORES.values())\n",
        "    print(f\"Task {task_name}: {SCORES[task_name]} / {max_points} points\")\n",
        "    print(f\"TOTAL POINTS: {total} / 100\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66cc2058",
      "metadata": {
        "id": "66cc2058"
      },
      "source": [
        "## 0. Imports and data loading\n",
        "\n",
        "In this homework we will use the classic **MNIST** dataset of handwritten digits (28√ó28, grayscale).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "97e1a757",
      "metadata": {
        "id": "97e1a757",
        "outputId": "758d92f4-d51b-46af-96c3-3864fbe4fa26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.19.0\n",
            "Train shape: (60000, 28, 28, 1) Test shape: (10000, 28, 28, 1)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# Load MNIST\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalize to [0, 1]\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "# Add channel dim: (N, 28, 28, 1)\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "\n",
        "num_classes = 10\n",
        "input_shape = x_train.shape[1:]\n",
        "\n",
        "print(\"Train shape:\", x_train.shape, \"Test shape:\", x_test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8dd31fb",
      "metadata": {
        "id": "e8dd31fb"
      },
      "source": [
        "## Task 1 (20 points): Basic TensorFlow tensors and operations\n",
        "\n",
        "Implement the function `tensor_stats(x)`.\n",
        "\n",
        "**Requirements:**\n",
        "\n",
        "- Input: 1D `tf.Tensor` of shape `(n,)` with `dtype=tf.float32`.\n",
        "- Return: a Python `dict` with keys:\n",
        "  - `\"mean\"` ‚Äî mean value\n",
        "  - `\"std\"` ‚Äî standard deviation\n",
        "  - `\"min\"` ‚Äî minimum\n",
        "  - `\"max\"` ‚Äî maximum\n",
        "- All values in the dict must be **Python floats** (not tensors).\n",
        "- Use **TensorFlow ops only**:\n",
        "  - `tf.reduce_mean`, `tf.math.reduce_std`, `tf.reduce_min`, `tf.reduce_max`\n",
        "- Do **not** convert `x` to NumPy inside the function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "fcfe3156",
      "metadata": {
        "id": "fcfe3156"
      },
      "outputs": [],
      "source": [
        "# TODO: implement tensor_stats\n",
        "def tensor_stats(x: tf.Tensor) -> dict:\n",
        "    \"\"\"Return basic statistics for a 1D float32 tensor.\n",
        "\n",
        "    Args:\n",
        "        x: tf.Tensor of shape (n,) and dtype float32.\n",
        "\n",
        "    Returns:\n",
        "        dict with keys 'mean', 'std', 'min', 'max' as Python floats.\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    mean = tf.reduce_mean(x)\n",
        "    std = tf.math.reduce_std(x)\n",
        "    min_v = tf.reduce_min(x)\n",
        "    max_v = tf.reduce_max(x)\n",
        "\n",
        "    # Convert to Python floats\n",
        "    return {\n",
        "        \"mean\": float(mean.numpy()),\n",
        "        \"std\": float(std.numpy()),\n",
        "        \"min\": float(min_v.numpy()),\n",
        "        \"max\": float(max_v.numpy()),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "67a32c33",
      "metadata": {
        "id": "67a32c33",
        "outputId": "5df601df-e1a0-4cf6-d229-7696a7b5b773",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task 1: 20 / 20 points\n",
            "TOTAL POINTS: 20 / 100\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# TEST CELL: Task 1 (20 points)\n",
        "max_points = 20\n",
        "points = 0\n",
        "\n",
        "try:\n",
        "    x = tf.constant([1.0, 2.0, 3.0, 4.0], dtype=tf.float32)\n",
        "    res = tensor_stats(x)\n",
        "except Exception as e:\n",
        "    print(\"Error when calling tensor_stats:\", e)\n",
        "    _set_score(\"1\", points, max_points)\n",
        "else:\n",
        "    # Basic checks\n",
        "    if isinstance(res, dict):\n",
        "        points += 5\n",
        "    else:\n",
        "        print(\"tensor_stats should return dict\")\n",
        "\n",
        "    for key in [\"mean\", \"std\", \"min\", \"max\"]:\n",
        "        if key in res and isinstance(res[key], float):\n",
        "            points += 3  # 4 keys * 3 = 12\n",
        "\n",
        "    # Numerical checks (simple)\n",
        "    if abs(res[\"mean\"] - 2.5) < 1e-5 and abs(res[\"min\"] - 1.0) < 1e-5 and abs(res[\"max\"] - 4.0) < 1e-5:\n",
        "        points += 3\n",
        "\n",
        "    _set_score(\"1\", points, max_points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0665e03c",
      "metadata": {
        "id": "0665e03c"
      },
      "source": [
        "## Task 2 (40 points): Dense neural network for MNIST with `tf.keras`\n",
        "\n",
        "Implement the function `build_dense_mnist_model(input_shape, num_classes)`.\n",
        "\n",
        "**Requirements:**\n",
        "\n",
        "- Use **`tf.keras` / `keras` only**.\n",
        "- Input: images of shape `input_shape` (e.g. `(28, 28, 1)`).\n",
        "- Output: probabilities over `num_classes` classes (use `softmax`).\n",
        "- Architecture:\n",
        "  - Flatten layer\n",
        "  - At least **one Dense hidden layer** with ‚â• 128 units and **ReLU** activation\n",
        "  - Final Dense layer with `num_classes` units and **softmax** activation\n",
        "- The model must be **compiled** with:\n",
        "  - Optimizer: `Adam` with learning rate `1e-3`\n",
        "  - Loss: `sparse_categorical_crossentropy`\n",
        "  - Metric: `accuracy`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "cd8ed3a0",
      "metadata": {
        "id": "cd8ed3a0"
      },
      "outputs": [],
      "source": [
        "# TODO: implement build_dense_mnist_model\n",
        "def build_dense_mnist_model(input_shape, num_classes):\n",
        "    \"\"\"Build and compile a dense neural network for MNIST.\n",
        "\n",
        "    Args:\n",
        "        input_shape: tuple, e.g. (28, 28, 1)\n",
        "        num_classes: int, number of classes (10 for MNIST)\n",
        "\n",
        "    Returns:\n",
        "        Compiled tf.keras.Model instance.\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    x = keras.layers.Flatten()(inputs)\n",
        "    x = keras.layers.Dense(128, activation=\"relu\")(x)\n",
        "    outputs = keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "e31bd325",
      "metadata": {
        "id": "e31bd325",
        "outputId": "de74b7ff-fb8a-4521-e720-8d6f2ba73211",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy after 3 epochs on subset: 0.8360000252723694\n",
            "Task 2: 30 / 40 points\n",
            "TOTAL POINTS: 50 / 100\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# TEST CELL: Task 2 (40 points)\n",
        "max_points = 40\n",
        "points = 0\n",
        "\n",
        "try:\n",
        "    model_t2 = build_dense_mnist_model(input_shape, num_classes)\n",
        "except Exception as e:\n",
        "    print(\"Error when calling build_dense_mnist_model:\", e)\n",
        "    _set_score(\"2\", points, max_points)\n",
        "else:\n",
        "    # Type check\n",
        "    if isinstance(model_t2, keras.Model):\n",
        "        points += 5\n",
        "\n",
        "    # Try a forward pass\n",
        "    try:\n",
        "        y_pred = model_t2(x_train[:32])\n",
        "        if y_pred.shape == (32, num_classes):\n",
        "            points += 5\n",
        "    except Exception as e:\n",
        "        print(\"Error in forward pass:\", e)\n",
        "\n",
        "    # Check softmax (probabilities close to 1 when summed)\n",
        "    s = tf.reduce_sum(y_pred[0]).numpy()\n",
        "    if abs(s - 1.0) < 1e-3:\n",
        "        points += 5\n",
        "\n",
        "    # Check that model is compiled (has optimizer, loss, metrics)\n",
        "    if model_t2.optimizer is not None and model_t2.loss is not None:\n",
        "        points += 5\n",
        "\n",
        "    # Train shortly on a small subset to see non-trivial accuracy\n",
        "    history = model_t2.fit(\n",
        "        x_train[:2000], y_train[:2000],\n",
        "        validation_data=(x_test[:1000], y_test[:1000]),\n",
        "        epochs=3,\n",
        "        batch_size=128,\n",
        "        verbose=0,\n",
        "    )\n",
        "\n",
        "    val_acc = history.history.get(\"val_accuracy\", [0])[-1]\n",
        "    print(\"Validation accuracy after 3 epochs on subset:\", val_acc)\n",
        "\n",
        "    # Reward reasonable accuracy\n",
        "    if val_acc > 0.85:\n",
        "        points += 20\n",
        "    elif val_acc > 0.75:\n",
        "        points += 10\n",
        "    elif val_acc > 0.65:\n",
        "        points += 5\n",
        "\n",
        "    _set_score(\"2\", points, max_points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82f13667",
      "metadata": {
        "id": "82f13667"
      },
      "source": [
        "## Task 3 (25 points): `tf.data` pipeline for MNIST\n",
        "\n",
        "Implement the function `make_mnist_dataset(x, y, batch_size)`.\n",
        "\n",
        "**Requirements:**\n",
        "\n",
        "- Create a `tf.data.Dataset` from NumPy arrays `x` and `y`.\n",
        "- Shuffle the dataset with a buffer size ‚â• `len(x) // 4`.\n",
        "- Batch with the given `batch_size`.\n",
        "- Prefetch with `tf.data.AUTOTUNE`.\n",
        "- Return the prepared dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "6793ec64",
      "metadata": {
        "id": "6793ec64"
      },
      "outputs": [],
      "source": [
        "# TODO: implement make_mnist_dataset\n",
        "def make_mnist_dataset(x, y, batch_size: int) -> tf.data.Dataset:\n",
        "    \"\"\"Create a shuffled, batched, prefetched tf.data.Dataset.\n",
        "\n",
        "    Args:\n",
        "        x: NumPy array of images.\n",
        "        y: NumPy array of labels.\n",
        "        batch_size: int, batch size.\n",
        "\n",
        "    Returns:\n",
        "        tf.data.Dataset yielding (batch_x, batch_y).\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    ds = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "    buffer_size = max(len(x) // 4, 1)\n",
        "    ds = ds.shuffle(buffer_size=buffer_size, seed=SEED)\n",
        "    ds = ds.batch(batch_size)\n",
        "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
        "    return ds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "88db6991",
      "metadata": {
        "id": "88db6991",
        "outputId": "352f2375-e0ed-4ebc-9d39-dae0d9aaf11b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy on 50 batches: 0.7115625143051147\n",
            "Task 3: 25 / 25 points\n",
            "TOTAL POINTS: 75 / 100\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# TEST CELL: Task 3 (25 points)\n",
        "max_points = 25\n",
        "points = 0\n",
        "\n",
        "try:\n",
        "    ds = make_mnist_dataset(x_train, y_train, batch_size=64)\n",
        "except Exception as e:\n",
        "    print(\"Error when calling make_mnist_dataset:\", e)\n",
        "    _set_score(\"3\", points, max_points)\n",
        "else:\n",
        "    if isinstance(ds, tf.data.Dataset):\n",
        "        points += 5\n",
        "\n",
        "    # Inspect one batch\n",
        "    for batch_x, batch_y in ds.take(1):\n",
        "        if batch_x.shape[0] <= 64 and batch_x.shape[1:] == input_shape:\n",
        "            points += 5\n",
        "        if batch_y.shape[0] == batch_x.shape[0]:\n",
        "            points += 5\n",
        "        break\n",
        "\n",
        "    spec = ds.element_spec\n",
        "    if isinstance(spec, tuple):\n",
        "        points += 5\n",
        "\n",
        "    # Try using in a small training loop\n",
        "    model_tmp = build_dense_mnist_model(input_shape, num_classes)\n",
        "    history = model_tmp.fit(ds.take(50), epochs=1, verbose=0)\n",
        "    train_acc = history.history.get(\"accuracy\", [0])[-1]\n",
        "    print(\"Train accuracy on 50 batches:\", train_acc)\n",
        "    if train_acc > 0.5:\n",
        "        points += 5\n",
        "\n",
        "    _set_score(\"3\", points, max_points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca2852de",
      "metadata": {
        "id": "ca2852de"
      },
      "source": [
        "## Task 4 (15 points): Custom training step with `tf.GradientTape`\n",
        "\n",
        "Implement the function `train_one_epoch(model, dataset, optimizer, loss_fn)` that performs **one epoch** of training\n",
        "with a custom loop.\n",
        "\n",
        "**Requirements:**\n",
        "\n",
        "- Iterate over `(x_batch, y_batch)` from `dataset`.\n",
        "- Use `tf.GradientTape()` to compute gradients of the loss w.r.t. **trainable variables**.\n",
        "- Apply gradients via `optimizer.apply_gradients(...)`.\n",
        "- Accumulate the mean loss for the epoch and return it as a Python float.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "5feea14e",
      "metadata": {
        "id": "5feea14e"
      },
      "outputs": [],
      "source": [
        "# TODO: implement train_one_epoch\n",
        "def train_one_epoch(model, dataset, optimizer, loss_fn) -> float:\n",
        "    \"\"\"Run one epoch of custom training loop.\n",
        "\n",
        "    Args:\n",
        "        model: tf.keras.Model\n",
        "        dataset: tf.data.Dataset yielding (x_batch, y_batch)\n",
        "        optimizer: tf.keras.optimizers.Optimizer\n",
        "        loss_fn: callable loss function\n",
        "\n",
        "    Returns:\n",
        "        Average loss over the epoch (Python float).\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "\n",
        "    for x_batch, y_batch in dataset:\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = model(x_batch, training=True)\n",
        "            loss_value = loss_fn(y_batch, logits)\n",
        "\n",
        "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        total_loss += float(loss_value.numpy())\n",
        "        num_batches += 1\n",
        "\n",
        "    if num_batches == 0:\n",
        "        return 0.0\n",
        "    return total_loss / num_batches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "da222e45",
      "metadata": {
        "id": "da222e45",
        "outputId": "8ddd1ff2-a603-408a-dea7-3a7bd04b78b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average loss over custom epoch: 1.7080150321125984\n",
            "Initial loss: 2.4094744 New loss: 0.9919338\n",
            "Task 4: 15 / 15 points\n",
            "TOTAL POINTS: 90 / 100\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# TEST CELL: Task 4 (15 points)\n",
        "max_points = 15\n",
        "points = 0\n",
        "\n",
        "try:\n",
        "    # Small model & dataset\n",
        "    model_small = build_dense_mnist_model(input_shape, num_classes)\n",
        "    ds_small = make_mnist_dataset(x_train[:2000], y_train[:2000], batch_size=128)\n",
        "    loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "    # Compute initial loss on one batch\n",
        "    x0, y0 = next(iter(ds_small))\n",
        "    initial_logits = model_small(x0, training=False)\n",
        "    initial_loss = loss_fn(y0, initial_logits).numpy()\n",
        "\n",
        "    avg_loss_epoch = train_one_epoch(model_small, ds_small.take(20), optimizer, loss_fn)\n",
        "    print(\"Average loss over custom epoch:\", avg_loss_epoch)\n",
        "\n",
        "    # Loss should be a float\n",
        "    if isinstance(avg_loss_epoch, float):\n",
        "        points += 5\n",
        "\n",
        "    # After one epoch, loss on the same batch should not increase a lot (ideally decrease)\n",
        "    new_logits = model_small(x0, training=False)\n",
        "    new_loss = loss_fn(y0, new_logits).numpy()\n",
        "    print(\"Initial loss:\", initial_loss, \"New loss:\", new_loss)\n",
        "\n",
        "    if new_loss <= initial_loss + 0.05:\n",
        "        points += 10\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Error in Task 4 test:\", e)\n",
        "\n",
        "_set_score(\"4\", points, max_points)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "d7b2d997",
      "metadata": {
        "id": "d7b2d997",
        "outputId": "dd728449-e57c-48b1-9914-5d6e13ab3181",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final SCORES dict: {'1': 20, '2': 30, '3': 25, '4': 15}\n",
            "If TOTAL POINTS above is 100, you have full score :)\n"
          ]
        }
      ],
      "source": [
        "print(\"Final SCORES dict:\", SCORES)\n",
        "print(\"If TOTAL POINTS above is 100, you have full score :)\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "# ---------- —Ç–µ—Å—Ç–æ–≤—ã–µ –ø–æ–º–æ—â–Ω–∏–∫–∏ ----------\n",
        "def add_points(ok, pts, msg_ok, msg_fail):\n",
        "    global total\n",
        "    if ok:\n",
        "        total += pts\n",
        "        print(f\"[+{pts:>2}] {msg_ok}\")\n",
        "    else:\n",
        "        print(f\"[ 0] {msg_fail}\")\n",
        "\n",
        "def _sec(td):\n",
        "    return td.total_seconds()\n",
        "\n",
        "def penalty_fraction(start_dt, due_dt, now_dt) -> float:\n",
        "    if not (start_dt and due_dt and now_dt):\n",
        "        return 0.0\n",
        "    window = _sec(due_dt - start_dt)\n",
        "    if window <= 0:\n",
        "        return 1.0 if now_dt > due_dt else 0.0\n",
        "    late = max(0.0, _sec(now_dt - due_dt))\n",
        "    return min(1.0, late / window)\n",
        "# ---------- –∞–≤—Ç–æ–≥—Ä–µ–π–¥–∏–Ω–≥ ----------\n",
        "total = points\n",
        "max_points = max_points\n",
        "raw_score = points\n",
        "\n",
        "import json\n",
        "\n",
        "# –ø—Ä–∏–º–µ–Ω—è–µ–º —à—Ç—Ä–∞—Ñ\n",
        "try:\n",
        "    pf = penalty_fraction(start_dt, due_dt, submission_dt)\n",
        "except NameError:\n",
        "    from datetime import timezone\n",
        "    pf = 0.0\n",
        "# ‚úÖ –ò—Ç–æ–≥–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
        "\n",
        "max_points = 100\n",
        "\n",
        "raw_score = round(100.0 * total / max_points)\n",
        "final_score = max(0.0, raw_score * (1.0 - min(1.0, pf)))\n",
        "\n",
        "print(f\"–°—ã—Ä–æ–π –±–∞–ª–ª: {raw_score}/{max_points}\")\n",
        "print(f\"–®—Ç—Ä–∞—Ñ (–¥–æ–ª—è): {pf:.4f}\")\n",
        "print(f\"–ò—Ç–æ–≥–æ–≤—ã–π –±–∞–ª–ª –ø–æ—Å–ª–µ —à—Ç—Ä–∞—Ñ–∞: {final_score:.2f}/{max_points}\")\n",
        "\n",
        "# –ü–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–æ–∫–∞ ‚Äî JSON, –∫–æ—Ç–æ—Ä—ã–π —á–∏—Ç–∞–µ—Ç harness\n",
        "final = {\n",
        "    \"name\": full_name,\n",
        "    \"group\": student_group,\n",
        "    \"assignment\": assignment_id,\n",
        "    \"score\": float(final_score)\n",
        "}\n"
      ],
      "metadata": {
        "id": "x5b19kKsd_9e",
        "outputId": "f0db3715-da7a-41c8-e1b3-6f4c471f3eb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "x5b19kKsd_9e",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–°—ã—Ä–æ–π –±–∞–ª–ª: 15/100\n",
            "–®—Ç—Ä–∞—Ñ (–¥–æ–ª—è): 0.0000\n",
            "–ò—Ç–æ–≥–æ–≤—ã–π –±–∞–ª–ª –ø–æ—Å–ª–µ —à—Ç—Ä–∞—Ñ–∞: 15.00/100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(json.dumps(final, ensure_ascii=False))"
      ],
      "metadata": {
        "id": "1k0UypmteFKU",
        "outputId": "461143de-2960-4d34-a70f-f177da5fb1b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "1k0UypmteFKU",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"name\": \"Doe John\", \"group\": \"11-111\", \"assignment\": \"HW_TENSORFLOW_BASICS\", \"score\": 15.0}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}