{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "190a4f42",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/alexander-toschev/ml-cs-intro/blob/main/home-work/HW_END_TO_END.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa479b2",
   "metadata": {},
   "source": [
    "# TensorFlow & tf.keras — Homework\n",
    "\n",
    "This notebook contains **auto-graded tasks** about core TensorFlow and the `tf.keras` high-level API.\n",
    "\n",
    "Fill in the cells marked with `# TODO` and run the **TEST CELL** below each task.\n",
    "\n",
    "- Language: English (code + comments)\n",
    "- Topic focus: **TensorFlow basics** + **end-to-end training with tf.keras**\n",
    "- Total: **100 points**\n",
    "\n",
    "After each test cell you will see:\n",
    "- Points for this task\n",
    "- Cumulative **TOTAL POINTS**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ffb3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 1) Student Info & Config\n",
    "# All code comments are in English.\n",
    "\n",
    "\n",
    "# === ОБЯЗАТЕЛЬНО ЗАПОЛНИТЬ ===\n",
    "full_name = \"Doe John\"     # например: \"Тощев Александр\"\n",
    "student_group = \"11-111\"      # например: \"208\"\n",
    "assignment_id = \"HW_TENSORFLOW_INTRO\"\n",
    "assert full_name != \"Фамилия Имя\", \"Заполните full_name\"\n",
    "assert student_group != \"Группа\", \"Заполните student_group\"\n",
    "print(\"✔ Student Info OK\")\n",
    "\n",
    "# Typical human accuracy (benchmark) for MNIST may be ~97-99%.\n",
    "\n",
    "print(\"Student:\", full_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711e59bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global score storage (do not modify)\n",
    "SCORES = {}\n",
    "\n",
    "def _set_score(task_name, points, max_points):\n",
    "    SCORES[task_name] = min(points, max_points)\n",
    "    total = sum(SCORES.values())\n",
    "    print(f\"Task {task_name}: {SCORES[task_name]} / {max_points} points\")\n",
    "    print(f\"TOTAL POINTS: {total} / 100\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cc2058",
   "metadata": {},
   "source": [
    "## 0. Imports and data loading\n",
    "\n",
    "In this homework we will use the classic **MNIST** dataset of handwritten digits (28×28, grayscale).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e1a757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Load MNIST\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize to [0, 1]\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Add channel dim: (N, 28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "num_classes = 10\n",
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "print(\"Train shape:\", x_train.shape, \"Test shape:\", x_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dd31fb",
   "metadata": {},
   "source": [
    "## Task 1 (20 points): Basic TensorFlow tensors and operations\n",
    "\n",
    "Implement the function `tensor_stats(x)`.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "- Input: 1D `tf.Tensor` of shape `(n,)` with `dtype=tf.float32`.\n",
    "- Return: a Python `dict` with keys:\n",
    "  - `\"mean\"` — mean value\n",
    "  - `\"std\"` — standard deviation\n",
    "  - `\"min\"` — minimum\n",
    "  - `\"max\"` — maximum\n",
    "- All values in the dict must be **Python floats** (not tensors).\n",
    "- Use **TensorFlow ops only**:\n",
    "  - `tf.reduce_mean`, `tf.math.reduce_std`, `tf.reduce_min`, `tf.reduce_max`\n",
    "- Do **not** convert `x` to NumPy inside the function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfe3156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement tensor_stats\n",
    "def tensor_stats(x: tf.Tensor) -> dict:\n",
    "    \"\"\"Return basic statistics for a 1D float32 tensor.\n",
    "\n",
    "    Args:\n",
    "        x: tf.Tensor of shape (n,) and dtype float32.\n",
    "\n",
    "    Returns:\n",
    "        dict with keys 'mean', 'std', 'min', 'max' as Python floats.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    mean = tf.reduce_mean(x)\n",
    "    std = tf.math.reduce_std(x)\n",
    "    min_v = tf.reduce_min(x)\n",
    "    max_v = tf.reduce_max(x)\n",
    "\n",
    "    # Convert to Python floats\n",
    "    return {\n",
    "        \"mean\": float(mean.numpy()),\n",
    "        \"std\": float(std.numpy()),\n",
    "        \"min\": float(min_v.numpy()),\n",
    "        \"max\": float(max_v.numpy()),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a32c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST CELL: Task 1 (20 points)\n",
    "max_points = 20\n",
    "points = 0\n",
    "\n",
    "try:\n",
    "    x = tf.constant([1.0, 2.0, 3.0, 4.0], dtype=tf.float32)\n",
    "    res = tensor_stats(x)\n",
    "except Exception as e:\n",
    "    print(\"Error when calling tensor_stats:\", e)\n",
    "    _set_score(\"1\", points, max_points)\n",
    "else:\n",
    "    # Basic checks\n",
    "    if isinstance(res, dict):\n",
    "        points += 5\n",
    "    else:\n",
    "        print(\"tensor_stats should return dict\")\n",
    "    \n",
    "    for key in [\"mean\", \"std\", \"min\", \"max\"]:\n",
    "        if key in res and isinstance(res[key], float):\n",
    "            points += 3  # 4 keys * 3 = 12\n",
    "    \n",
    "    # Numerical checks (simple)\n",
    "    if abs(res[\"mean\"] - 2.5) < 1e-5 and abs(res[\"min\"] - 1.0) < 1e-5 and abs(res[\"max\"] - 4.0) < 1e-5:\n",
    "        points += 3\n",
    "\n",
    "    _set_score(\"1\", points, max_points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0665e03c",
   "metadata": {},
   "source": [
    "## Task 2 (40 points): Dense neural network for MNIST with `tf.keras`\n",
    "\n",
    "Implement the function `build_dense_mnist_model(input_shape, num_classes)`.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "- Use **`tf.keras` / `keras` only**.\n",
    "- Input: images of shape `input_shape` (e.g. `(28, 28, 1)`).\n",
    "- Output: probabilities over `num_classes` classes (use `softmax`).\n",
    "- Architecture:\n",
    "  - Flatten layer\n",
    "  - At least **one Dense hidden layer** with ≥ 128 units and **ReLU** activation\n",
    "  - Final Dense layer with `num_classes` units and **softmax** activation\n",
    "- The model must be **compiled** with:\n",
    "  - Optimizer: `Adam` with learning rate `1e-3`\n",
    "  - Loss: `sparse_categorical_crossentropy`\n",
    "  - Metric: `accuracy`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8ed3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement build_dense_mnist_model\n",
    "def build_dense_mnist_model(input_shape, num_classes):\n",
    "    \"\"\"Build and compile a dense neural network for MNIST.\n",
    "\n",
    "    Args:\n",
    "        input_shape: tuple, e.g. (28, 28, 1)\n",
    "        num_classes: int, number of classes (10 for MNIST)\n",
    "\n",
    "    Returns:\n",
    "        Compiled tf.keras.Model instance.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = keras.layers.Flatten()(inputs)\n",
    "    x = keras.layers.Dense(128, activation=\"relu\")(x)\n",
    "    outputs = keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31bd325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST CELL: Task 2 (40 points)\n",
    "max_points = 40\n",
    "points = 0\n",
    "\n",
    "try:\n",
    "    model_t2 = build_dense_mnist_model(input_shape, num_classes)\n",
    "except Exception as e:\n",
    "    print(\"Error when calling build_dense_mnist_model:\", e)\n",
    "    _set_score(\"2\", points, max_points)\n",
    "else:\n",
    "    # Type check\n",
    "    if isinstance(model_t2, keras.Model):\n",
    "        points += 5\n",
    "\n",
    "    # Try a forward pass\n",
    "    try:\n",
    "        y_pred = model_t2(x_train[:32])\n",
    "        if y_pred.shape == (32, num_classes):\n",
    "            points += 5\n",
    "    except Exception as e:\n",
    "        print(\"Error in forward pass:\", e)\n",
    "\n",
    "    # Check softmax (probabilities close to 1 when summed)\n",
    "    s = tf.reduce_sum(y_pred[0]).numpy()\n",
    "    if abs(s - 1.0) < 1e-3:\n",
    "        points += 5\n",
    "\n",
    "    # Check that model is compiled (has optimizer, loss, metrics)\n",
    "    if model_t2.optimizer is not None and model_t2.loss is not None:\n",
    "        points += 5\n",
    "\n",
    "    # Train shortly on a small subset to see non-trivial accuracy\n",
    "    history = model_t2.fit(\n",
    "        x_train[:2000], y_train[:2000],\n",
    "        validation_data=(x_test[:1000], y_test[:1000]),\n",
    "        epochs=3,\n",
    "        batch_size=128,\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    val_acc = history.history.get(\"val_accuracy\", [0])[-1]\n",
    "    print(\"Validation accuracy after 3 epochs on subset:\", val_acc)\n",
    "\n",
    "    # Reward reasonable accuracy\n",
    "    if val_acc > 0.85:\n",
    "        points += 20\n",
    "    elif val_acc > 0.75:\n",
    "        points += 10\n",
    "    elif val_acc > 0.65:\n",
    "        points += 5\n",
    "\n",
    "    _set_score(\"2\", points, max_points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f13667",
   "metadata": {},
   "source": [
    "## Task 3 (25 points): `tf.data` pipeline for MNIST\n",
    "\n",
    "Implement the function `make_mnist_dataset(x, y, batch_size)`.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "- Create a `tf.data.Dataset` from NumPy arrays `x` and `y`.\n",
    "- Shuffle the dataset with a buffer size ≥ `len(x) // 4`.\n",
    "- Batch with the given `batch_size`.\n",
    "- Prefetch with `tf.data.AUTOTUNE`.\n",
    "- Return the prepared dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6793ec64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement make_mnist_dataset\n",
    "def make_mnist_dataset(x, y, batch_size: int) -> tf.data.Dataset:\n",
    "    \"\"\"Create a shuffled, batched, prefetched tf.data.Dataset.\n",
    "\n",
    "    Args:\n",
    "        x: NumPy array of images.\n",
    "        y: NumPy array of labels.\n",
    "        batch_size: int, batch size.\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset yielding (batch_x, batch_y).\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    ds = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    buffer_size = max(len(x) // 4, 1)\n",
    "    ds = ds.shuffle(buffer_size=buffer_size, seed=SEED)\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88db6991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST CELL: Task 3 (25 points)\n",
    "max_points = 25\n",
    "points = 0\n",
    "\n",
    "try:\n",
    "    ds = make_mnist_dataset(x_train, y_train, batch_size=64)\n",
    "except Exception as e:\n",
    "    print(\"Error when calling make_mnist_dataset:\", e)\n",
    "    _set_score(\"3\", points, max_points)\n",
    "else:\n",
    "    if isinstance(ds, tf.data.Dataset):\n",
    "        points += 5\n",
    "\n",
    "    # Inspect one batch\n",
    "    for batch_x, batch_y in ds.take(1):\n",
    "        if batch_x.shape[0] <= 64 and batch_x.shape[1:] == input_shape:\n",
    "            points += 5\n",
    "        if batch_y.shape[0] == batch_x.shape[0]:\n",
    "            points += 5\n",
    "        break\n",
    "\n",
    "    spec = ds.element_spec\n",
    "    if isinstance(spec, tuple):\n",
    "        points += 5\n",
    "\n",
    "    # Try using in a small training loop\n",
    "    model_tmp = build_dense_mnist_model(input_shape, num_classes)\n",
    "    history = model_tmp.fit(ds.take(50), epochs=1, verbose=0)\n",
    "    train_acc = history.history.get(\"accuracy\", [0])[-1]\n",
    "    print(\"Train accuracy on 50 batches:\", train_acc)\n",
    "    if train_acc > 0.5:\n",
    "        points += 5\n",
    "\n",
    "    _set_score(\"3\", points, max_points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2852de",
   "metadata": {},
   "source": [
    "## Task 4 (15 points): Custom training step with `tf.GradientTape`\n",
    "\n",
    "Implement the function `train_one_epoch(model, dataset, optimizer, loss_fn)` that performs **one epoch** of training\n",
    "with a custom loop.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "- Iterate over `(x_batch, y_batch)` from `dataset`.\n",
    "- Use `tf.GradientTape()` to compute gradients of the loss w.r.t. **trainable variables**.\n",
    "- Apply gradients via `optimizer.apply_gradients(...)`.\n",
    "- Accumulate the mean loss for the epoch and return it as a Python float.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5feea14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement train_one_epoch\n",
    "def train_one_epoch(model, dataset, optimizer, loss_fn) -> float:\n",
    "    \"\"\"Run one epoch of custom training loop.\n",
    "\n",
    "    Args:\n",
    "        model: tf.keras.Model\n",
    "        dataset: tf.data.Dataset yielding (x_batch, y_batch)\n",
    "        optimizer: tf.keras.optimizers.Optimizer\n",
    "        loss_fn: callable loss function\n",
    "\n",
    "    Returns:\n",
    "        Average loss over the epoch (Python float).\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for x_batch, y_batch in dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch, training=True)\n",
    "            loss_value = loss_fn(y_batch, logits)\n",
    "\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        total_loss += float(loss_value.numpy())\n",
    "        num_batches += 1\n",
    "\n",
    "    if num_batches == 0:\n",
    "        return 0.0\n",
    "    return total_loss / num_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da222e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST CELL: Task 4 (15 points)\n",
    "max_points = 15\n",
    "points = 0\n",
    "\n",
    "try:\n",
    "    # Small model & dataset\n",
    "    model_small = build_dense_mnist_model(input_shape, num_classes)\n",
    "    ds_small = make_mnist_dataset(x_train[:2000], y_train[:2000], batch_size=128)\n",
    "    loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "    # Compute initial loss on one batch\n",
    "    x0, y0 = next(iter(ds_small))\n",
    "    initial_logits = model_small(x0, training=False)\n",
    "    initial_loss = loss_fn(y0, initial_logits).numpy()\n",
    "\n",
    "    avg_loss_epoch = train_one_epoch(model_small, ds_small.take(20), optimizer, loss_fn)\n",
    "    print(\"Average loss over custom epoch:\", avg_loss_epoch)\n",
    "\n",
    "    # Loss should be a float\n",
    "    if isinstance(avg_loss_epoch, float):\n",
    "        points += 5\n",
    "\n",
    "    # After one epoch, loss on the same batch should not increase a lot (ideally decrease)\n",
    "    new_logits = model_small(x0, training=False)\n",
    "    new_loss = loss_fn(y0, new_logits).numpy()\n",
    "    print(\"Initial loss:\", initial_loss, \"New loss:\", new_loss)\n",
    "\n",
    "    if new_loss <= initial_loss + 0.05:\n",
    "        points += 10\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error in Task 4 test:\", e)\n",
    "\n",
    "_set_score(\"4\", points, max_points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b2d997",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final SCORES dict:\", SCORES)\n",
    "print(\"If TOTAL POINTS above is 100, you have full score :)\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
