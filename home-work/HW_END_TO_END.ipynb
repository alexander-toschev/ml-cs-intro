{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexander-toschev/ml-cs-intro/blob/main/home-work/HW_END_TO_END.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "id": "6bf170cc",
      "cell_type": "markdown",
      "source": [
        "# End-to-End Deep Learning with Keras ‚Äî Homework\n",
        "\n",
        "This notebook contains **auto-graded tasks** about end-to-end deep learning with Keras.\n",
        "\n",
        "Fill in the cells marked with `# TODO` and run the test cells below each task.\n",
        "\n",
        "- Language: English (code + comments)\n",
        "- Topic focus: **end-to-end deep learning** with CNNs in Keras\n",
        "- Total: **100 points**\n",
        "\n",
        "After each test cell you will see:\n",
        "- Points for this task\n",
        "- Cumulative **TOTAL POINTS**\n"
      ],
      "metadata": {
        "id": "6bf170cc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1) Student Info & Config\n",
        "# All code comments are in English.\n",
        "\n",
        "\n",
        "# === –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –ó–ê–ü–û–õ–ù–ò–¢–¨ ===\n",
        "full_name = \"Doe John\"     # –Ω–∞–ø—Ä–∏–º–µ—Ä: \"–¢–æ—â–µ–≤ –ê–ª–µ–∫—Å–∞–Ω–¥—Ä\"\n",
        "student_group = \"11-111\"      # –Ω–∞–ø—Ä–∏–º–µ—Ä: \"208\"\n",
        "assignment_id = \"HW_END-TO_END\"\n",
        "assert full_name != \"–§–∞–º–∏–ª–∏—è –ò–º—è\", \"–ó–∞–ø–æ–ª–Ω–∏—Ç–µ full_name\"\n",
        "assert student_group != \"–ì—Ä—É–ø–ø–∞\", \"–ó–∞–ø–æ–ª–Ω–∏—Ç–µ student_group\"\n",
        "print(\"‚úî Student Info OK\")\n",
        "\n",
        "# Typical human accuracy (benchmark) for MNIST may be ~97-99%.\n",
        "\n",
        "print(\"Student:\", full_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jfEs9a4jwev",
        "outputId": "0bd7c02e-d209-4c48-ba0f-529009415919"
      },
      "id": "9jfEs9a4jwev",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úî Student Info OK\n",
            "Student: Doe John\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "def _close(a, b, tol=1e-8):\n",
        "    return np.allclose(a, b, atol=tol)\n",
        "\n",
        "def _arr_equal(a, b):\n",
        "    a = np.array(a)\n",
        "    b = np.array(b)\n",
        "    return a.shape == b.shape and np.array_equal(a, b)"
      ],
      "metadata": {
        "id": "9fHg-vh8jyvh"
      },
      "id": "9fHg-vh8jyvh",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime, timezone, timedelta\n",
        "\n",
        "# –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –æ–∫–Ω–∞ –ø—Ä–∏—ë–º–∞ (–ø—Ä–∏–º–µ—Ä):\n",
        "\n",
        "start_at_iso = \"2025-11-24T09:00-04:00\"  #@param {type:\"string\"}\n",
        "due_at_iso   = \"2025-12-01T23:59-04:00\"  #@param {type:\"string\"}\n",
        "start_dt = datetime.fromisoformat(start_at_iso)\n",
        "due_dt   = datetime.fromisoformat(due_at_iso)\n",
        "# –î–ª—è –ø—Ä–æ—Ç–æ–∫–æ–ª–∞: –≤—Ä–µ–º—è —Å–¥–∞—á–∏ –±–µ—Ä—ë–º —Ç–µ–∫—É—â–µ–µ (–º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ mtime —Ñ–∞–π–ª–∞)\n",
        "import os\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "# üìÖ Add submission date based on file modification time\n",
        "try:\n",
        "    nb_path = __file__ if \"__file__\" in globals() else \"EM01_Numpy.ipynb\"\n",
        "    mtime = os.path.getmtime(nb_path)\n",
        "    submission_dt = datetime.fromtimestamp(mtime, tz=timezone.utc)\n",
        "except Exception:\n",
        "    submission_dt = datetime.utcnow().replace(tzinfo=timezone.utc)\n",
        "\n",
        "def penalty_fraction(start_dt, due_dt, submission_dt):\n",
        "    \"\"\"–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–æ–ª—é —à—Ç—Ä–∞—Ñ–∞ [0..1].\n",
        "    0 ‚Äî –±–µ–∑ —à—Ç—Ä–∞—Ñ–∞ (<= due_dt). –õ–∏–Ω–µ–π–Ω–æ —Ä–∞—Å—Ç—ë—Ç –æ—Ç due_dt –∫ due_dt + (due_dt - start_dt).\n",
        "    –ù–µ –≤—ã—Ö–æ–¥–∏—Ç –∑–∞ 1.0.\n",
        "    \"\"\"\n",
        "    if submission_dt <= due_dt:\n",
        "        return 0.0\n",
        "    total = (due_dt - start_dt).total_seconds()\n",
        "    late  = (submission_dt - due_dt).total_seconds()\n",
        "    if total <= 0:\n",
        "        return 1.0 if late > 0 else 0.0\n",
        "    return min(1.0, max(0.0, late / total))\n",
        "\n",
        "print(f\"–û–∫–Ω–æ –ø—Ä–∏—ë–º–∞: {start_dt.isoformat()} ‚Äî {due_dt.isoformat()} (UTC)\")\n",
        "print(f\"–í—Ä–µ–º—è —Å–¥–∞—á–∏: {submission_dt.isoformat()} (UTC)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPkpfiYWj1Rm",
        "outputId": "bd05af9c-8653-4787-f9ba-6873fe391674"
      },
      "id": "CPkpfiYWj1Rm",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–û–∫–Ω–æ –ø—Ä–∏—ë–º–∞: 2025-11-24T09:00:00-04:00 ‚Äî 2025-12-01T23:59:00-04:00 (UTC)\n",
            "–í—Ä–µ–º—è —Å–¥–∞—á–∏: 2025-12-01T15:38:30.979113+00:00 (UTC)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-632059888.py:19: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  submission_dt = datetime.utcnow().replace(tzinfo=timezone.utc)\n"
          ]
        }
      ]
    },
    {
      "id": "6bef7a88",
      "cell_type": "code",
      "metadata": {
        "id": "6bef7a88"
      },
      "execution_count": 4,
      "source": [
        "# Global score storage (do not modify)\n",
        "SCORES = {}\n",
        "\n",
        "def _set_score(task_name, points, max_points):\n",
        "    SCORES[task_name] = min(points, max_points)\n",
        "    total = sum(SCORES.values())\n",
        "    print(f\"Task {task_name}: {SCORES[task_name]} / {max_points} points\")\n",
        "    print(f\"TOTAL POINTS: {total} / 100\\n\")\n"
      ],
      "outputs": []
    },
    {
      "id": "ad5221f0",
      "cell_type": "markdown",
      "source": [
        "## 0. Imports and data loading\n",
        "\n",
        "In this homework we will use **Fashion-MNIST** as an example dataset.\n",
        "It is slightly harder than MNIST but still small enough to train quickly in Colab."
      ],
      "metadata": {
        "id": "ad5221f0"
      }
    },
    {
      "id": "9c404069",
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c404069",
        "outputId": "2010a7a3-7e8a-4200-a082-da573bf9c2f0"
      },
      "execution_count": 5,
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "# Load Fashion-MNIST\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "# Normalize to [0, 1]\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Add channel dimension: (N, 28, 28, 1)\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "\n",
        "num_classes = 10\n",
        "input_shape = x_train.shape[1:]\n",
        "\n",
        "print('Train shape:', x_train.shape)\n",
        "print('Test shape:', x_test.shape)\n",
        "print('Input shape:', input_shape)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.19.0\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Train shape: (60000, 28, 28, 1)\n",
            "Test shape: (10000, 28, 28, 1)\n",
            "Input shape: (28, 28, 1)\n"
          ]
        }
      ]
    },
    {
      "id": "6cfb6c94",
      "cell_type": "markdown",
      "source": [
        "## Task 1 (40 points): Build an end-to-end CNN model\n",
        "\n",
        "Implement the function `build_e2e_cnn(input_shape, num_classes)`.\n",
        "\n",
        "Requirements:\n",
        "- Use `tf.keras` / `keras` only.\n",
        "- The model must be **end-to-end**:\n",
        "  - Input: raw normalized images of shape `input_shape` (e.g., `(28, 28, 1)`).\n",
        "  - Output: logits or probabilities over `num_classes` classes.\n",
        "- Architecture constraints:\n",
        "  - Use at least **2 convolutional layers** (`Conv2D`).\n",
        "  - Use at least **1 pooling layer** (`MaxPooling2D`).\n",
        "  - Use at least **1 Dense hidden layer** before the final output layer.\n",
        "- The final layer must have `num_classes` units.\n",
        "- The model must be **compiled** with:\n",
        "  - optimizer: `'adam'`\n",
        "  - loss: `'sparse_categorical_crossentropy'`\n",
        "  - metric: `'accuracy'`\n",
        "\n",
        "Feel free to change the number of filters/units, as long as the constraints above are satisfied."
      ],
      "metadata": {
        "id": "6cfb6c94"
      }
    },
    {
      "id": "166e1a29",
      "cell_type": "code",
      "metadata": {
        "id": "166e1a29"
      },
      "execution_count": 6,
      "source": [
        "# TODO: implement build_e2e_cnn\n",
        "def build_e2e_cnn(input_shape, num_classes):\n",
        "    \"\"\"Build and compile an end-to-end CNN model.\n",
        "\n",
        "    Args:\n",
        "        input_shape: tuple, e.g. (28, 28, 1)\n",
        "        num_classes: int, e.g. 10\n",
        "\n",
        "    Returns:\n",
        "        A compiled tf.keras.Model instance.\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError\n"
      ],
      "outputs": []
    },
    {
      "id": "b4b6334e",
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4b6334e",
        "outputId": "e1191567-8241-4a21-861f-ea698ded802c"
      },
      "execution_count": 7,
      "source": [
        "# TEST CELL: Task 1 (40 points)\n",
        "max_points = 40\n",
        "points = 0\n",
        "\n",
        "try:\n",
        "    model_t1 = build_e2e_cnn(input_shape, num_classes)\n",
        "except Exception as e:\n",
        "    print('Error when calling build_e2e_cnn:', e)\n",
        "    _set_score('1', points, max_points)\n",
        "else:\n",
        "    # Check type\n",
        "    if isinstance(model_t1, keras.Model):\n",
        "        points += 5\n",
        "    else:\n",
        "        print('Model is not an instance of keras.Model')\n",
        "\n",
        "    # Check compilation\n",
        "    if model_t1.optimizer is not None:\n",
        "        points += 5\n",
        "\n",
        "    # Check loss and metrics\n",
        "    loss = getattr(model_t1, 'loss', None)\n",
        "    metrics = getattr(model_t1, 'metrics', [])\n",
        "    if isinstance(loss, str) and 'sparse_categorical_crossentropy' in loss:\n",
        "        points += 5\n",
        "    elif hasattr(loss, '__name__') and 'sparse_categorical_crossentropy' in loss.__name__:\n",
        "        points += 5\n",
        "    else:\n",
        "        print('Unexpected loss function (should be sparse_categorical_crossentropy).')\n",
        "\n",
        "    metric_names = [m.name for m in metrics if hasattr(m, 'name')]\n",
        "    if 'accuracy' in metric_names or 'acc' in metric_names:\n",
        "        points += 5\n",
        "    else:\n",
        "        print('No accuracy metric found.')\n",
        "\n",
        "    # Check final layer size\n",
        "    try:\n",
        "        output_units = model_t1.output_shape[-1]\n",
        "        if output_units == num_classes:\n",
        "            points += 10\n",
        "        else:\n",
        "            print(f'Output units {output_units} != num_classes {num_classes}')\n",
        "    except Exception as e:\n",
        "        print('Error checking output shape:', e)\n",
        "\n",
        "    # Check presence of Conv2D, MaxPooling2D, Dense\n",
        "    layer_types = [type(l) for l in model_t1.layers]\n",
        "    conv_count = sum(issubclass(t, layers.Conv2D) for t in layer_types)\n",
        "    pool_count = sum(issubclass(t, layers.MaxPooling2D) for t in layer_types)\n",
        "    dense_count = sum(issubclass(t, layers.Dense) for t in layer_types)\n",
        "\n",
        "    if conv_count >= 2:\n",
        "        points += 5\n",
        "    else:\n",
        "        print('Model must have at least 2 Conv2D layers.')\n",
        "\n",
        "    if pool_count >= 1:\n",
        "        points += 3\n",
        "    else:\n",
        "        print('Model must have at least 1 MaxPooling2D layer.')\n",
        "\n",
        "    if dense_count >= 2:  # hidden + output\n",
        "        points += 2\n",
        "    else:\n",
        "        print('Model must have at least 1 hidden Dense layer + output Dense layer.')\n",
        "\n",
        "    _set_score('1', points, max_points)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error when calling build_e2e_cnn: \n",
            "Task 1: 0 / 40 points\n",
            "TOTAL POINTS: 0 / 100\n",
            "\n"
          ]
        }
      ]
    },
    {
      "id": "a0598c06",
      "cell_type": "markdown",
      "source": [
        "## Task 2 (30 points): Train the end-to-end CNN\n",
        "\n",
        "Now implement `train_e2e_cnn` that trains your model on Fashion-MNIST.\n",
        "\n",
        "Requirements:\n",
        "- Split the training set into train/validation using `validation_split` or manual split.\n",
        "- Train for **at least 3 epochs**.\n",
        "- Return the **History** object from `model.fit`.\n",
        "\n",
        "Do **not** hard-code global variables inside the function body (use arguments)."
      ],
      "metadata": {
        "id": "a0598c06"
      }
    },
    {
      "id": "c20bc5fa",
      "cell_type": "code",
      "metadata": {
        "id": "c20bc5fa"
      },
      "execution_count": 8,
      "source": [
        "# TODO: implement train_e2e_cnn\n",
        "def train_e2e_cnn(model, x_train, y_train, epochs=3, batch_size=64, validation_split=0.1):\n",
        "    \"\"\"Train the end-to-end CNN model on Fashion-MNIST.\n",
        "\n",
        "    Args:\n",
        "        model: compiled keras.Model\n",
        "        x_train, y_train: training data and labels\n",
        "        epochs: number of epochs (>= 3)\n",
        "        batch_size: batch size\n",
        "        validation_split: fraction of training data for validation\n",
        "\n",
        "    Returns:\n",
        "        history: the History object returned by model.fit\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError\n"
      ],
      "outputs": []
    },
    {
      "id": "85deafa8",
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85deafa8",
        "outputId": "e2fcaa8e-94e5-403c-f885-8d3db3878711"
      },
      "execution_count": 9,
      "source": [
        "# TEST CELL: Task 2 (30 points)\n",
        "max_points = 30\n",
        "points = 0\n",
        "\n",
        "try:\n",
        "    model_t2 = build_e2e_cnn(input_shape, num_classes)\n",
        "    history_t2 = train_e2e_cnn(model_t2, x_train, y_train, epochs=3, batch_size=128, validation_split=0.1)\n",
        "except Exception as e:\n",
        "    print('Error in train_e2e_cnn or during training:', e)\n",
        "    _set_score('2', points, max_points)\n",
        "else:\n",
        "    # Check type\n",
        "    if hasattr(history_t2, 'history'):\n",
        "        points += 10\n",
        "    else:\n",
        "        print('Returned object does not look like a Keras History.')\n",
        "\n",
        "    # Check presence of accuracy and loss in logs\n",
        "    h = history_t2.history\n",
        "    keys = h.keys()\n",
        "    if 'loss' in keys and any(k.startswith('val_') and 'loss' in k for k in keys):\n",
        "        points += 5\n",
        "    else:\n",
        "        print('No loss/val_loss in history.')\n",
        "\n",
        "    if 'accuracy' in keys or 'acc' in keys:\n",
        "        points += 5\n",
        "    else:\n",
        "        print('No accuracy in history.')\n",
        "\n",
        "    # Check number of epochs\n",
        "    epochs_run = len(h.get('loss', []))\n",
        "    if epochs_run >= 3:\n",
        "        points += 10\n",
        "    else:\n",
        "        print(f'Not enough epochs trained: {epochs_run} < 3')\n",
        "\n",
        "    _set_score('2', points, max_points)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in train_e2e_cnn or during training: \n",
            "Task 2: 0 / 30 points\n",
            "TOTAL POINTS: 0 / 100\n",
            "\n"
          ]
        }
      ]
    },
    {
      "id": "7727e215",
      "cell_type": "markdown",
      "source": [
        "## Task 3 (30 points): Decide if a pipeline is end-to-end\n",
        "\n",
        "Implement the function `is_end_to_end` that decides whether a given pipeline is **end-to-end deep learning**.\n",
        "\n",
        "The pipeline is described by three boolean flags:\n",
        "- `has_raw_input_to_model`: `True` if the model takes raw data (e.g. pixels, waveforms) as input, not hand-crafted features.\n",
        "- `has_separate_manual_feature_stage`: `True` if there is a separate, hand-written feature extraction stage **outside** the neural network.\n",
        "- `has_multiple_independent_models`: `True` if the pipeline consists of several independently trained models chained together (not one jointly trained model).\n",
        "\n",
        "Rules (for this homework):\n",
        "- A pipeline is **end-to-end** if:\n",
        "  - `has_raw_input_to_model` is `True`, AND\n",
        "  - `has_separate_manual_feature_stage` is `False`, AND\n",
        "  - `has_multiple_independent_models` is `False`.\n",
        "- Otherwise it is **not** end-to-end.\n",
        "\n",
        "Return `True` if the pipeline is end-to-end, otherwise `False`."
      ],
      "metadata": {
        "id": "7727e215"
      }
    },
    {
      "id": "acf5d0be",
      "cell_type": "code",
      "metadata": {
        "id": "acf5d0be"
      },
      "execution_count": 10,
      "source": [
        "# TODO: implement is_end_to_end\n",
        "def is_end_to_end(has_raw_input_to_model: bool,\n",
        "                  has_separate_manual_feature_stage: bool,\n",
        "                  has_multiple_independent_models: bool) -> bool:\n",
        "    \"\"\"Return True if the pipeline is end-to-end deep learning (according to the rules above).\"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError\n"
      ],
      "outputs": []
    },
    {
      "id": "d72d6f27",
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d72d6f27",
        "outputId": "2e77f16a-4ade-48ab-c34a-e88b934c3a1d"
      },
      "execution_count": 11,
      "source": [
        "# TEST CELL: Task 3 (30 points)\n",
        "max_points = 30\n",
        "points = 0\n",
        "\n",
        "try:\n",
        "    # 1) Pure end-to-end: raw input, no manual features, single model\n",
        "    if is_end_to_end(True, False, False) is True:\n",
        "        points += 10\n",
        "    else:\n",
        "        print('Case 1 failed: should be end-to-end.')\n",
        "\n",
        "    # 2) Manual features present -> not end-to-end\n",
        "    if is_end_to_end(True, True, False) is False:\n",
        "        points += 5\n",
        "    else:\n",
        "        print('Case 2 failed: manual feature stage -> not end-to-end.')\n",
        "\n",
        "    # 3) Multiple independently trained models -> not end-to-end\n",
        "    if is_end_to_end(True, False, True) is False:\n",
        "        points += 5\n",
        "    else:\n",
        "        print('Case 3 failed: multiple independent models -> not end-to-end.')\n",
        "\n",
        "    # 4) No raw input to model -> not end-to-end\n",
        "    if is_end_to_end(False, False, False) is False:\n",
        "        points += 5\n",
        "    else:\n",
        "        print('Case 4 failed: no raw input to model -> not end-to-end.')\n",
        "\n",
        "    # 5) All bad flags -> not end-to-end\n",
        "    if is_end_to_end(False, True, True) is False:\n",
        "        points += 5\n",
        "    else:\n",
        "        print('Case 5 failed: all flags true -> not end-to-end.')\n",
        "\n",
        "except Exception as e:\n",
        "    print('Error when calling is_end_to_end:', e)\n",
        "\n",
        "_set_score('3', points, max_points)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error when calling is_end_to_end: \n",
            "Task 3: 0 / 30 points\n",
            "TOTAL POINTS: 0 / 100\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "max_total = max_points\n",
        "total_points=points\n",
        "print(f\"Final score: {total_points}/{max_total}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzvRNqvfkC-z",
        "outputId": "86f77396-2334-4d72-9c32-1afffdea9f7a"
      },
      "id": "kzvRNqvfkC-z",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final score: 0/30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "# ---------- —Ç–µ—Å—Ç–æ–≤—ã–µ –ø–æ–º–æ—â–Ω–∏–∫–∏ ----------\n",
        "def add_points(ok, pts, msg_ok, msg_fail):\n",
        "    global total\n",
        "    if ok:\n",
        "        total += pts\n",
        "        print(f\"[+{pts:>2}] {msg_ok}\")\n",
        "    else:\n",
        "        print(f\"[ 0] {msg_fail}\")\n",
        "\n",
        "def _sec(td):\n",
        "    return td.total_seconds()\n",
        "\n",
        "def penalty_fraction(start_dt, due_dt, now_dt) -> float:\n",
        "    if not (start_dt and due_dt and now_dt):\n",
        "        return 0.0\n",
        "    window = _sec(due_dt - start_dt)\n",
        "    if window <= 0:\n",
        "        return 1.0 if now_dt > due_dt else 0.0\n",
        "    late = max(0.0, _sec(now_dt - due_dt))\n",
        "    return min(1.0, late / window)\n",
        "# ---------- –∞–≤—Ç–æ–≥—Ä–µ–π–¥–∏–Ω–≥ ----------\n",
        "total = 0\n",
        "max_points = max_total\n",
        "raw_score = total_points\n",
        "\n",
        "import json\n",
        "\n",
        "# –ø—Ä–∏–º–µ–Ω—è–µ–º —à—Ç—Ä–∞—Ñ\n",
        "try:\n",
        "    pf = penalty_fraction(start_dt, due_dt, submission_dt)\n",
        "except NameError:\n",
        "    from datetime import timezone\n",
        "    pf = 0.0\n",
        "# ‚úÖ –ò—Ç–æ–≥–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
        "\n",
        "max_points = 100\n",
        "\n",
        "raw_score = round(100.0 * total / max_points)\n",
        "final_score = max(0.0, raw_score * (1.0 - min(1.0, pf)))\n",
        "\n",
        "print(f\"–°—ã—Ä–æ–π –±–∞–ª–ª: {raw_score}/{max_points}\")\n",
        "print(f\"–®—Ç—Ä–∞—Ñ (–¥–æ–ª—è): {pf:.4f}\")\n",
        "print(f\"–ò—Ç–æ–≥–æ–≤—ã–π –±–∞–ª–ª –ø–æ—Å–ª–µ —à—Ç—Ä–∞—Ñ–∞: {final_score:.2f}/{max_points}\")\n",
        "\n",
        "# –ü–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–æ–∫–∞ ‚Äî JSON, –∫–æ—Ç–æ—Ä—ã–π —á–∏—Ç–∞–µ—Ç harness\n",
        "final = {\n",
        "    \"name\": full_name,\n",
        "    \"group\": student_group,\n",
        "    \"assignment\": assignment_id,\n",
        "    \"score\": float(final_score)\n",
        "}\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDrsvcM3j5Sf",
        "outputId": "5e6b0b45-daaf-463a-f43f-8cac5cb8ae41"
      },
      "id": "hDrsvcM3j5Sf",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–°—ã—Ä–æ–π –±–∞–ª–ª: 0/100\n",
            "–®—Ç—Ä–∞—Ñ (–¥–æ–ª—è): 0.0000\n",
            "–ò—Ç–æ–≥–æ–≤—ã–π –±–∞–ª–ª –ø–æ—Å–ª–µ —à—Ç—Ä–∞—Ñ–∞: 0.00/100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(json.dumps(final, ensure_ascii=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bx1g87-Pj-6I",
        "outputId": "30a79963-1c35-466b-f508-627ef8084388"
      },
      "id": "Bx1g87-Pj-6I",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"name\": \"Doe John\", \"group\": \"11-111\", \"assignment\": \"EM02_Keras\", \"score\": 0.0}\n"
          ]
        }
      ]
    },
    {
      "id": "a4facf3a",
      "cell_type": "markdown",
      "source": [
        "## Final note\n",
        "\n",
        "You can re-run the test cells after changing your solutions.\n",
        "The final **TOTAL POINTS** is printed after each test cell.\n",
        "\n",
        "Good luck!"
      ],
      "metadata": {
        "id": "a4facf3a"
      }
    }
  ]
}