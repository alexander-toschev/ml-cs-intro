{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eabc206c",
   "metadata": {},
   "source": [
    "# üß™ Multi-Task Learning Assignment (Vision, PyTorch)\n",
    "\n",
    "Auto-grading is enabled. Fill in the first block. Where indicated, **do not change variable names**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2946c9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 1) Student Info & Config\n",
    "# All code comments are in English.\n",
    "\n",
    "# === MUST FILL ===\n",
    "full_name = \"Doe John\"        # e.g., \"Toshchev Alexander\"\n",
    "student_group = \"11-111\"      # e.g., \"208\"\n",
    "assignment_id = \"HW_MTL_01\"\n",
    "assert full_name != \"–§–∞–º–∏–ª–∏—è –ò–º—è\", \"–ó–∞–ø–æ–ª–Ω–∏—Ç–µ full_name\"\n",
    "assert student_group != \"–ì—Ä—É–ø–ø–∞\", \"–ó–∞–ø–æ–ª–Ω–∏—Ç–µ student_group\"\n",
    "print(\"‚úî Student Info OK\")\n",
    "\n",
    "# Typical human accuracy (reference) for simple image classification could be high,\n",
    "# but we use this only as a narrative target in reports.\n",
    "HUMAN_ACCURACY = 98.0  # @param {type:\"number\"}\n",
    "\n",
    "print(\"Student:\", full_name)\n",
    "print(\"Human reference accuracy (%):\", HUMAN_ACCURACY)\n",
    "\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "# Windows for submissions (example):\n",
    "start_at_iso = \"2025-10-20T09:00-04:00\"  # @param {type:\"string\"}\n",
    "due_at_iso   = \"2025-11-03T23:59-04:00\"  # @param {type:\"string\"}\n",
    "start_dt = datetime.fromisoformat(start_at_iso)\n",
    "due_dt   = datetime.fromisoformat(due_at_iso)\n",
    "\n",
    "# For the protocol: take current time as submission (or mtime of the notebook file)\n",
    "import os\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# üìÖ Add submission date based on file modification time\n",
    "try:\n",
    "    nb_path = __file__ if \"__file__\" in globals() else \"MTL_Assignment.ipynb\"\n",
    "    mtime = os.path.getmtime(nb_path)\n",
    "    submission_dt = datetime.fromtimestamp(mtime, tz=timezone.utc)\n",
    "except Exception:\n",
    "    submission_dt = datetime.utcnow().replace(tzinfo=timezone.utc)\n",
    "\n",
    "def penalty_fraction(start_dt, due_dt, submission_dt):\n",
    "    \"\"\"Returns penalty fraction in [0..1].\n",
    "    0 ‚Üí no penalty (<= due_dt). Grows linearly from due_dt to due_dt + (due_dt - start_dt).\n",
    "    Clamped to 1.0.\n",
    "    \"\"\"\n",
    "    if submission_dt <= due_dt:\n",
    "        return 0.0\n",
    "    total = (due_dt - start_dt).total_seconds()\n",
    "    late  = (submission_dt - due_dt).total_seconds()\n",
    "    if total <= 0:\n",
    "        return 1.0 if late > 0 else 0.0\n",
    "    return min(1.0, max(0.0, late / total))\n",
    "\n",
    "print(f\"Window: {start_dt.isoformat()} ‚Äî {due_dt.isoformat()} (UTC)\")\n",
    "print(f\"Submitted at: {submission_dt.isoformat()} (UTC)\")\n",
    "\n",
    "# raw score accumulator\n",
    "raw_score = 0.0\n",
    "max_points = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606a836a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 2) Environment Check\n",
    "import torch, torchvision\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Torchvision:\", torchvision.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3234ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 3) Setup & Utilities\n",
    "import torch, torch.nn as nn, torch.optim as optim, torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import random, time, json, math, os\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "NUM_WORKERS = 2\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS_STAGE = 3  # quick stage; feel free to increase later\n",
    "SEED = 42\n",
    "random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "def count_trainable_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_mtl(trunk, head_cls, head_rot, dl):\n",
    "    trunk.eval(); head_cls.eval(); head_rot.eval()\n",
    "    total, correct_cls, correct_rot = 0, 0, 0\n",
    "    for x, y_cls, y_rot in dl:\n",
    "        x, y_cls, y_rot = x.to(DEVICE), y_cls.to(DEVICE), y_rot.to(DEVICE)\n",
    "        f = trunk(x)\n",
    "        logits_cls = head_cls(f)\n",
    "        logits_rot = head_rot(f)\n",
    "        pred_cls = logits_cls.argmax(1)\n",
    "        pred_rot = logits_rot.argmax(1)\n",
    "        correct_cls += (pred_cls == y_cls).sum().item()\n",
    "        correct_rot += (pred_rot == y_rot).sum().item()\n",
    "        total += y_cls.size(0)\n",
    "    return correct_cls/total, correct_rot/total\n",
    "\n",
    "def train_epoch_mtl(trunk, head_cls, head_rot, dl, opt, sched, criterion, weights=None, uncertainty=None):\n",
    "    trunk.train(); head_cls.train(); head_rot.train()\n",
    "    if weights is None: weights = (1.0, 1.0)\n",
    "    w_cls, w_rot = weights\n",
    "    if uncertainty is not None:\n",
    "        log_vars = uncertainty  # nn.Parameter([logœÉ¬≤_cls, logœÉ¬≤_rot])\n",
    "\n",
    "    for x, y_cls, y_rot in dl:\n",
    "        x, y_cls, y_rot = x.to(DEVICE), y_cls.to(DEVICE), y_rot.to(DEVICE)\n",
    "        opt.zero_grad()\n",
    "        f = trunk(x)\n",
    "        logits_cls = head_cls(f)\n",
    "        logits_rot = head_rot(f)\n",
    "        loss_cls = criterion(logits_cls, y_cls)\n",
    "        loss_rot = criterion(logits_rot, y_rot)\n",
    "        if uncertainty is None:\n",
    "            loss = w_cls * loss_cls + w_rot * loss_rot\n",
    "        else:\n",
    "            # Uncertainty weighting (Kendall & Gal)\n",
    "            loss = torch.exp(-log_vars[0]) * loss_cls + log_vars[0] \\\n",
    "                 + torch.exp(-log_vars[1]) * loss_rot + log_vars[1]\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(list(trunk.parameters())+list(head_cls.parameters())+list(head_rot.parameters()), max_norm=5.0)\n",
    "        opt.step()\n",
    "        if sched is not None: sched.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a0f969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 4) Data: CIFAR10 + Rotation Task\n",
    "# Multi-task setup: Task A = CIFAR10 classification (10 classes)\n",
    "# Task B = Rotation prediction with 4 bins {0¬∞, 90¬∞, 180¬∞, 270¬∞}\n",
    "\n",
    "from torchvision import transforms, datasets\n",
    "import torch\n",
    "\n",
    "IMG_SIZE = 224\n",
    "\n",
    "base_train_tfms = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.7, 1.0)),\n",
    "    transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "base_val_tfms = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE + 32),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_base = datasets.CIFAR10(root='./data', train=True, download=True, transform=base_train_tfms)\n",
    "val_base   = datasets.CIFAR10(root='./data', train=False, download=True, transform=base_val_tfms)\n",
    "\n",
    "def rotate_batch(x, y_cls):\n",
    "    # rotate by a random choice among [0, 90, 180, 270]\n",
    "    b = x.size(0)\n",
    "    rots = torch.randint(0, 4, (b,))  # 0..3\n",
    "    # Apply rotation\n",
    "    x_out = []\n",
    "    for i in range(b):\n",
    "        img = x[i]\n",
    "        r = int(rots[i].item())\n",
    "        x_out.append(torch.rot90(img, r, dims=(1,2)))\n",
    "    return torch.stack(x_out, dim=0), y_cls, rots\n",
    "\n",
    "class RotWrapper(Dataset):\n",
    "    def __init__(self, base_ds):\n",
    "        self.base = base_ds\n",
    "    def __len__(self): return len(self.base)\n",
    "    def __getitem__(self, idx):\n",
    "        img, y = self.base[idx]\n",
    "        return img, y\n",
    "\n",
    "def collate_with_rotate(batch):\n",
    "    xs, ys = zip(*batch)\n",
    "    x = torch.stack(xs, dim=0)\n",
    "    y_cls = torch.tensor(ys, dtype=torch.long)\n",
    "    x_rot, y_cls, y_rot = rotate_batch(x, y_cls)\n",
    "    return x_rot, y_cls, y_rot\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "train_ds = RotWrapper(train_base)\n",
    "val_ds   = RotWrapper(val_base)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "NUM_WORKERS = 2\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True, collate_fn=collate_with_rotate)\n",
    "val_dl   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True, collate_fn=collate_with_rotate)\n",
    "\n",
    "num_classes = 10\n",
    "num_rot_bins = 4\n",
    "print(\"Train/Val sizes:\", len(train_ds), len(val_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a251bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 5) Model: Shared Trunk + Two Heads\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "class Trunk(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        base = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        self.features = nn.Sequential(*list(base.children())[:-1])  # [B, 512, 1, 1]\n",
    "        self.out_dim = base.fc.in_features\n",
    "    def forward(self, x):\n",
    "        f = self.features(x)\n",
    "        return f.view(f.size(0), -1)\n",
    "\n",
    "class HeadCls(nn.Module):\n",
    "    def __init__(self, in_dim, ncls):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_dim, ncls)\n",
    "    def forward(self, f): return self.fc(f)\n",
    "\n",
    "class HeadRot(nn.Module):\n",
    "    def __init__(self, in_dim, nbins):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_dim, nbins)\n",
    "    def forward(self, f): return self.fc(f)\n",
    "\n",
    "trunk = Trunk().to(DEVICE)\n",
    "head_cls = HeadCls(trunk.out_dim, num_classes).to(DEVICE)\n",
    "head_rot = HeadRot(trunk.out_dim, num_rot_bins).to(DEVICE)\n",
    "\n",
    "def count_trainable_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Trainable params (total):\", count_trainable_params(trunk)+count_trainable_params(head_cls)+count_trainable_params(head_rot))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd94d82b",
   "metadata": {},
   "source": [
    "## 6) Task 1 ‚Äî **Hard Sharing, Equal Weights** (max 25 pts)\n",
    "Train the shared trunk with two heads. Use equal loss weights (1.0, 1.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cd2bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Run Task 1 (Equal Weights)\n",
    "import torch.nn as nn, torch.optim as optim\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "opt = optim.AdamW(list(trunk.parameters())+list(head_cls.parameters())+list(head_rot.parameters()), lr=1e-3, weight_decay=1e-4)\n",
    "sched = OneCycleLR(opt, max_lr=1e-3, epochs=EPOCHS_STAGE, steps_per_epoch=max(1, len(train_dl)))\n",
    "\n",
    "def evaluate_mtl(trunk, head_cls, head_rot, dl):\n",
    "    trunk.eval(); head_cls.eval(); head_rot.eval()\n",
    "    total, correct_cls, correct_rot = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y_cls, y_rot in dl:\n",
    "            x, y_cls, y_rot = x.to(DEVICE), y_cls.to(DEVICE), y_rot.to(DEVICE)\n",
    "            f = trunk(x)\n",
    "            logits_cls = head_cls(f)\n",
    "            logits_rot = head_rot(f)\n",
    "            pred_cls = logits_cls.argmax(1)\n",
    "            pred_rot = logits_rot.argmax(1)\n",
    "            correct_cls += (pred_cls == y_cls).sum().item()\n",
    "            correct_rot += (pred_rot == y_rot).sum().item()\n",
    "            total += y_cls.size(0)\n",
    "    return correct_cls/total, correct_rot/total\n",
    "\n",
    "def train_epoch_mtl(trunk, head_cls, head_rot, dl, opt, sched, criterion, weights=None, uncertainty=None):\n",
    "    trunk.train(); head_cls.train(); head_rot.train()\n",
    "    if weights is None: weights = (1.0, 1.0)\n",
    "    w_cls, w_rot = weights\n",
    "    if uncertainty is not None:\n",
    "        log_vars = uncertainty\n",
    "    for x, y_cls, y_rot in dl:\n",
    "        x, y_cls, y_rot = x.to(DEVICE), y_cls.to(DEVICE), y_rot.to(DEVICE)\n",
    "        opt.zero_grad()\n",
    "        f = trunk(x)\n",
    "        logits_cls = head_cls(f)\n",
    "        logits_rot = head_rot(f)\n",
    "        loss_cls = criterion(logits_cls, y_cls)\n",
    "        loss_rot = criterion(logits_rot, y_rot)\n",
    "        if uncertainty is None:\n",
    "            loss = w_cls * loss_cls + w_rot * loss_rot\n",
    "        else:\n",
    "            loss = torch.exp(-log_vars[0]) * loss_cls + log_vars[0] \\\n",
    "                 + torch.exp(-log_vars[1]) * loss_rot + log_vars[1]\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(list(trunk.parameters())+list(head_cls.parameters())+list(head_rot.parameters()), max_norm=5.0)\n",
    "        opt.step()\n",
    "        if sched is not None: sched.step()\n",
    "\n",
    "for ep in range(1, EPOCHS_STAGE+1):\n",
    "    train_epoch_mtl(trunk, head_cls, head_rot, train_dl, opt, sched, criterion, weights=(1.0, 1.0))\n",
    "    acc_cls, acc_rot = evaluate_mtl(trunk, head_cls, head_rot, val_dl)\n",
    "    print(f\"[T1] Epoch {ep}/{EPOCHS_STAGE} | val_acc_cls={acc_cls:.4f} val_acc_rot={acc_rot:.4f}\")\n",
    "\n",
    "t1_pts = 0\n",
    "if acc_cls >= 0.55 and acc_rot >= 0.55: t1_pts = 25\n",
    "elif acc_cls >= 0.45 and acc_rot >= 0.50: t1_pts = 18\n",
    "elif acc_cls >= 0.35 and acc_rot >= 0.45: t1_pts = 12\n",
    "elif acc_cls >= 0.30 and acc_rot >= 0.40: t1_pts = 8\n",
    "else: t1_pts = 4\n",
    "\n",
    "raw_score += t1_pts\n",
    "print(f\"Task1 ‚Üí +{t1_pts} pts (raw_score={raw_score})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ab1c61",
   "metadata": {},
   "source": [
    "## 7) Task 2 ‚Äî **Manual Reweighting** (max 25 pts)\n",
    "Re-run with manual weights to mitigate conflict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7461de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Run Task 2 (Manual Reweighting)\n",
    "trunk2 = Trunk().to(DEVICE)\n",
    "head_cls2 = HeadCls(trunk2.out_dim, num_classes).to(DEVICE)\n",
    "head_rot2 = HeadRot(trunk2.out_dim, num_rot_bins).to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "opt2 = optim.AdamW(list(trunk2.parameters())+list(head_cls2.parameters())+list(head_rot2.parameters()), lr=1e-3, weight_decay=1e-4)\n",
    "sched2 = OneCycleLR(opt2, max_lr=1e-3, epochs=EPOCHS_STAGE, steps_per_epoch=max(1, len(train_dl)))\n",
    "\n",
    "W_CLS = 1.0  # @param {type:\"number\"}\n",
    "W_ROT = 0.5  # @param {type:\"number\"}\n",
    "\n",
    "for ep in range(1, EPOCHS_STAGE+1):\n",
    "    train_epoch_mtl(trunk2, head_cls2, head_rot2, train_dl, opt2, sched2, criterion, weights=(W_CLS, W_ROT))\n",
    "    acc_cls2, acc_rot2 = evaluate_mtl(trunk2, head_cls2, head_rot2, val_dl)\n",
    "    print(f\"[T2] Epoch {ep}/{EPOCHS_STAGE} | val_acc_cls={acc_cls2:.4f} val_acc_rot={acc_rot2:.4f}\")\n",
    "\n",
    "t2_pts = 0\n",
    "improve = (acc_cls2 >= acc_cls + 0.02) or (acc_rot2 >= acc_rot + 0.02)\n",
    "not_collapse = (acc_cls2 >= acc_cls - 0.05) and (acc_rot2 >= acc_rot - 0.05)\n",
    "\n",
    "if improve and not_collapse: t2_pts = 25\n",
    "elif not_collapse: t2_pts = 16\n",
    "else: t2_pts = 8\n",
    "\n",
    "raw_score += t2_pts\n",
    "print(f\"Task2 ‚Üí +{t2_pts} pts (raw_score={raw_score})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2546f1",
   "metadata": {},
   "source": [
    "## 8) Task 3 ‚Äî **Uncertainty Weighting** (max 30 pts)\n",
    "Learn log-variances to auto-balance tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458bdca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Run Task 3 (Uncertainty Weighting)\n",
    "trunk3 = Trunk().to(DEVICE)\n",
    "head_cls3 = HeadCls(trunk3.out_dim, num_classes).to(DEVICE)\n",
    "head_rot3 = HeadRot(trunk3.out_dim, num_rot_bins).to(DEVICE)\n",
    "\n",
    "log_vars = nn.Parameter(torch.zeros(2, device=DEVICE))  # [logœÉ¬≤_cls, logœÉ¬≤_rot]\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "opt3 = optim.AdamW(list(trunk3.parameters())+list(head_cls3.parameters())+list(head_rot3.parameters())+[log_vars], lr=1e-3, weight_decay=1e-4)\n",
    "sched3 = OneCycleLR(opt3, max_lr=1e-3, epochs=EPOCHS_STAGE, steps_per_epoch=max(1, len(train_dl)))\n",
    "\n",
    "for ep in range(1, EPOCHS_STAGE+1):\n",
    "    train_epoch_mtl(trunk3, head_cls3, head_rot3, train_dl, opt3, sched3, criterion, uncertainty=log_vars)\n",
    "    acc_cls3, acc_rot3 = evaluate_mtl(trunk3, head_cls3, head_rot3, val_dl)\n",
    "    print(f\"[T3] Epoch {ep}/{EPOCHS_STAGE} | val_acc_cls={acc_cls3:.4f} val_acc_rot={acc_rot3:.4f} | log_vars={log_vars.detach().cpu().numpy()}\")\n",
    "\n",
    "t3_pts = 0\n",
    "if (acc_cls3 >= max(acc_cls, 0.55)) and (acc_rot3 >= max(acc_rot, 0.55)):\n",
    "    t3_pts = 30\n",
    "elif (acc_cls3 >= acc_cls or acc_rot3 >= acc_rot):\n",
    "    t3_pts = 20\n",
    "else:\n",
    "    t3_pts = 10\n",
    "\n",
    "raw_score += t3_pts\n",
    "print(f\"Task3 ‚Üí +{t3_pts} pts (raw_score={raw_score})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773eb948",
   "metadata": {},
   "source": [
    "## 9) Task 4 ‚Äî **Soft Sharing (Sketch)** ‚Äî Bonus up to +10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dd624a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Bonus (manual)\n",
    "bonus_points = 0.0  # set 0..10 after implementing a soft-sharing variant and documenting\n",
    "raw_score = min(100.0, raw_score + float(bonus_points))\n",
    "print(\"raw_score (with optional bonus, capped at 100) ‚Üí\", raw_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1162ffa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 10) Summary of Results\n",
    "summary = {\n",
    "    \"task1_equal_weights\": {\"acc_cls\": float(acc_cls),  \"acc_rot\": float(acc_rot)},\n",
    "    \"task2_manual\":        {\"acc_cls\": float(acc_cls2), \"acc_rot\": float(acc_rot2)},\n",
    "    \"task3_uncertainty\":   {\"acc_cls\": float(acc_cls3), \"acc_rot\": float(acc_rot3)},\n",
    "    \"epochs_per_stage\": int(EPOCHS_STAGE),\n",
    "    \"device\": DEVICE,\n",
    "}\n",
    "print(json.dumps(summary, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e8b227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 11) Finalize & Grade (Penalty + JSON)\n",
    "import json\n",
    "\n",
    "# apply penalty\n",
    "try:\n",
    "    pf = penalty_fraction(start_dt, due_dt, submission_dt)\n",
    "except NameError:\n",
    "    from datetime import timezone\n",
    "    pf = 0.0\n",
    "# ‚úÖ Final score\n",
    "max_points=100\n",
    "final_score = max(0.0, raw_score * (1.0 - min(1.0, pf)))\n",
    "\n",
    "print(f\"–°—ã—Ä–æ–π –±–∞–ª–ª: {raw_score}/{max_points}\")\n",
    "print(f\"–®—Ç—Ä–∞—Ñ (–¥–æ–ª—è): {pf:.4f}\")\n",
    "print(f\"–ò—Ç–æ–≥–æ–≤—ã–π –±–∞–ª–ª –ø–æ—Å–ª–µ —à—Ç—Ä–∞—Ñ–∞: {final_score:.2f}/{max_points}\")\n",
    "\n",
    "# Last line ‚Äî JSON for the harness\n",
    "final = {\n",
    "    \"name\": full_name,\n",
    "    \"group\": student_group,\n",
    "    \"assignment\": assignment_id,\n",
    "    \"score\": float(final_score)\n",
    "}\n",
    "\n",
    "print(json.dumps(final, ensure_ascii=False))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
